# -*- coding: utf-8 -*-
"""sgpt2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rY64FowJ21RCj0ncGfrYfSEFCVTqSBOi

Get cntkillme's Roblox Forums posts
"""

import requests
from bs4 import BeautifulSoup

page = requests.get("https://archive.froast.io/users/294568/asc")
soup = BeautifulSoup(page.content, "html.parser")
posts = []
next_button = soup.find("a", class_="Forward Link-Enabled")
# turn this into a loop until there are no more pages
while next_button is not None:
    # get all posts
    for tr in soup.find_all("tr")[1::2]:
        topic_element = tr.find("div", class_="Topic")
        body_element = tr.find("div", class_="Body")

        if topic_element is not None and body_element is not None:
            topic = topic_element.text
            body = body_element.text
            posts.append(topic + "\n\ncntkillme:\n\n" + body + "\n\n")

    # get next page
    next_button = soup.find("a", class_="Forward Link-Enabled")
    if next_button is not None and len(posts) < 4000:
        url = "https://archive.froast.io" + next_button["href"]
        if len(posts)
        page = requests.get(url)
        soup = BeautifulSoup(page.content, "html.parser")
    else:
        break

print(posts[:10])

# # write to file
# with open("corpora/cntkillme.txt", "w", encoding="utf-8") as f:
#     f.writelines(posts)

"""Import and fine-tune GPT-2 on the data"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install tiktoken
import tiktoken

enc = tiktoken.get_encoding("gpt2")

train_ids = enc.encode_ordinary(train_data_str)
val_ids = enc.encode_ordinary(val_data_str)

import numpy as np
import os

data_dir = 'friends_gpt2'
os.makedirs(data_dir, exist_ok=True)

# export to bin files
train_ids = np.array(train_ids, dtype=np.uint16)
val_ids = np.array(val_ids, dtype=np.uint16)
train_ids.tofile(os.path.join(data_dir, 'train.bin'))
val_ids.tofile(os.path.join(data_dir, 'val.bin'))

# create a memory map
train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')
val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')

import torch

def get_batch(data, block_size, batch_size, device):
    """
    Return a minibatch of data. This function is not deterministic.
    Calling this function multiple times will result in multiple different
    return values.

    Parameters:
        `data` - a numpy array (e.g., created via a call to np.memmap)
        `block_size` - the length of each sequence
        `batch_size` - the number of sequences in the batch
        `device` - the device to place the returned PyTorch tensor

    Returns: A tuple of PyTorch tensors (x, t), where
        `x` - represents the input tokens, with shape (batch_size, block_size)
        `y` - represents the target output tokens, with shape (batch_size, block_size)
    """
    ix = torch.randint(len(data) - block_size, (batch_size,))
    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])
    t = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])
    if 'cuda' in device:
        # pin arrays x,t, which allows us to move them to GPU asynchronously
        #  (non_blocking=True)
        x, t = x.pin_memory().to(device, non_blocking=True), t.pin_memory().to(device, non_blocking=True)
    else:
        x, t = x.to(device), t.to(device)
    return x, t